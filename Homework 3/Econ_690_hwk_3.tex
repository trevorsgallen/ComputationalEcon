\documentclass[11pt]{article}
\usepackage{appendix}
\usepackage{graphicx} 
\usepackage{setspace}
\usepackage{amsmath,float,verbatim,multicol}
\usepackage{array}
\usepackage{lscape}
\usepackage{hyperref}

\usepackage{amssymb}
\bibliographystyle{plainnat}
\usepackage[round]{natbib}
\usepackage{multirow}

\setlength{\textheight}{8.8in} \setlength{\textwidth}{6.3in}
\setlength{\oddsidemargin}{0.2in} \setlength{\topmargin}{-0.30in}
\setlength{\footnotesep}{10.0pt}

\newcommand{\ol}{\overline}



\renewcommand{\baselinestretch}{1.25}
\title{Bayesian Estimation of a Structural Model using Markov chain Monte Carlo (Metropolis-Hastings Algorithm)  }
\author{ Trevor Gallen \\ Econ 64200 }
\date{Fall 2023}

\begin{document}
\bibliographystyle{myplainnat}
%\bibpunct{(}{)}{;}{a}{}{,}6868

\maketitle

This homework asks you to write down and run a simple dynamic problem using reinforcement learning.
\textbf{Deliverables}
\begin{itemize}
\item You should have a word/\LaTeX document that has three sections: 
\begin{enumerate}
\item Discusses the model and answers the questions I pose throughout.
\item Contains the tables and figures you will produce.
\item Contains a discussion of your programming choices if you had to make any.
\end{enumerate}
\item You should have a Matlab file or set of files (zipped) that contain \textbf{all} your programs and raw data.  There should be a file called ``Main.M" that produces everything I need in one click.
\end{itemize}


\section{Model}
Each period, firms begin the period with prices $p_{t-1}$ and observe optimal price $\overline{P}_t$.  They may either change their price $p_t$, which costs $\phi(p_{t}-p_{t-1})^2$, or leave it fixed at $p_{t-1}$.  Their value function is:
$$V(p_{t-1},\overline{P}_t)=\underset{p_t}{\max} -(p_t-\overline{P}_t)^2-\phi(p_{t}-p_{t-1})^2+\beta V(p_{t-1},\overline{P}_t)$$
Where:
$$\overline{P}_t=\rho \overline{P}_{t-1}+\epsilon\ \ \ \epsilon\sim\mathcal{N}\left(0,\sigma^2\right)$$
Let $\rho=0.95$, $\sigma=0.01$, and $\phi=0.1$.  
\ \\
\ \\

\section{Problems}
\textbf{Question 1} Do the following in matlab:
\begin{itemize}
\item Define an environment for the problem: observation (2x1) and action info (1x1).
\item Write out a reset function for state variables $p_{t-1}$ and $\overline{P}_t$
\item Write out a step function that takes in $\overline{P}_t$, $p_{t-1}$ and $p_{t}$ and generates the reward and next  $\overline{P}_{t+1}$
\item Define a critic network, and actor network, and an agent
\item Train the agent
\item Display the agent's optimal policy function for $p_t$ as a function of $\overline{P}_t$ and $p_{t-1}$.
\end{itemize}
 
\end{document}





